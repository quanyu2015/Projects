{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle competition: [Allstate Claims Severity](https://www.kaggle.com/c/allstate-claims-severity)\n",
    "Introduction:\n",
    "> When you’ve been devastated by a serious car accident, your focus is on the things that matter the most: family, friends, and other loved ones. Pushing paper with your insurance agent is the last place you want your time or mental energy spent. This is why [Allstate](https://www.allstate.com/), a personal insurer in the United States, is continually seeking fresh ideas to improve their claims service for the over 16 million households they protect.\n",
    "\n",
    "> Allstate is currently developing automated methods of predicting the cost, and hence severity, of claims. In this recruitment challenge, Kagglers are invited to show off their creativity and flex their technical chops by creating an algorithm which accurately predicts claims severity. Aspiring competitors will demonstrate insight into better ways to predict claims severity for the chance to be part of Allstate’s efforts to ensure a worry-free customer experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188318, 132)\n",
      "(125546, 131)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.44467</td>\n",
       "      <td>0.327915</td>\n",
       "      <td>0.321570</td>\n",
       "      <td>0.605077</td>\n",
       "      <td>0.602642</td>\n",
       "      <td>939.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.21230</td>\n",
       "      <td>0.204687</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>2763.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9   ...        cont6  \\\n",
       "0   1    A    B    A    B    A    A    A    A    B   ...     0.718367   \n",
       "1   2    A    B    A    A    A    A    A    A    B   ...     0.438917   \n",
       "2   5    A    B    A    A    B    A    A    A    B   ...     0.289648   \n",
       "3  10    B    B    A    B    A    A    A    A    B   ...     0.440945   \n",
       "4  11    A    B    A    B    A    A    A    A    B   ...     0.178193   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
       "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "3  0.602642   939.85  \n",
       "4  0.432606  2763.85  \n",
       "\n",
       "[5 rows x 132 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")\n",
    "\n",
    "print(dataset.shape)\n",
    "print(test.shape)\n",
    "\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create dummy variables for both training and testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(313864, 130)\n",
      "188318 125546 313864\n",
      "(188318, 1139)\n",
      "(313864, 1176)\n",
      "(313864, 14)\n",
      "(313864, 1190)\n",
      "(188318, 1190)\n",
      "(125546, 1190)\n"
     ]
    }
   ],
   "source": [
    "loss = dataset.iloc[:, 131]\n",
    "\n",
    "id_train = dataset['id']\n",
    "id_test = test['id']\n",
    "n1 = len(dataset.index)\n",
    "n2 = len(test.index)\n",
    "\n",
    "# For some columns, labels in test data are different from train data, so that\n",
    "# we should combine train and test data, create dummy vars, and then split train and test\n",
    "dataset = dataset.iloc[:,1:131]\n",
    "test    = test.iloc[:, 1:131]\n",
    "dfnew = dataset.append(test, ignore_index=True)\n",
    "print(dfnew.shape)\n",
    "n3 = len(dfnew.index)\n",
    "print n1, n2, n3\n",
    "\n",
    "df0 = pd.get_dummies(dataset.iloc[:, 0:116])\n",
    "print(df0.shape)\n",
    "df1 = pd.get_dummies(dfnew.iloc[:, 0:116])\n",
    "df2 = dfnew.iloc[:, 116:131]\n",
    "print(df1.shape)\n",
    "print(df2.shape)\n",
    "# we found that the combined dataset has more dummy variables than training dataset\n",
    "\n",
    "dfnew = pd.concat([df1, df2], axis = 1)\n",
    "print(dfnew.shape)\n",
    "\n",
    "train = dfnew.iloc[0:n1, :]\n",
    "test  = dfnew.iloc[n1:n3, :]\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for following model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169486, 1190)\n",
      "(18832, 1190)\n"
     ]
    }
   ],
   "source": [
    "# pandas dataframe to numpy array\n",
    "X = train.values\n",
    "Y = loss.values\n",
    "Y = np.log1p(Y)\n",
    "\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# split train data for cross validation\n",
    "val_size = 0.1\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=val_size, random_state=0)\n",
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the performance of Lasso regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 1262.53144301\n",
      "0.002 1266.5983432\n",
      "0.005 1289.94823958\n",
      "0.01 1344.3820347\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "for alpha in [0.001, 0.002, 0.005, 0.01]:\n",
    "  model = Lasso(alpha=alpha, random_state=0, max_iter = 500)\n",
    "  model.fit(X_train, Y_train)\n",
    "  Y_pred = model.predict(X_test)\n",
    "  print alpha, mean_absolute_error(np.expm1(Y_test), np.expm1(Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then test the popular Extreme Gradient Boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 1219.71291158\n",
      "200 1193.30103586\n",
      "500 1173.67955071\n",
      "1000 1168.79391021\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "for n_estimators in [100, 200, 500, 1000]:\n",
    "  model = XGBRegressor(n_estimators=n_estimators,seed=0)\n",
    "  model.fit(X_train, Y_train)\n",
    "  Y_pred = model.predict(X_test)\n",
    "  print n_estimators, mean_absolute_error(np.expm1(Y_test), np.expm1(Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see if we can remove some variables to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169486, 595) (18832, 595)\n",
      "50% 1168.59042176\n",
      "(169486, 892) (18832, 892)\n",
      "75% 1166.80152502\n",
      "(169486, 1130) (18832, 1130)\n",
      "95% 1169.96463204\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "\n",
    "# use F regression to select different percentage of variables\n",
    "for pt in [50, 75, 95]:\n",
    "  selector = SelectPercentile(f_regression, percentile= pt)\n",
    "  selector.fit(X_train, Y_train)\n",
    "\n",
    "  X_t1 = selector.transform(X_train)\n",
    "  X_t2 = selector.transform(X_test)\n",
    "  print X_t1.shape, X_t2.shape\n",
    "\n",
    "  model = XGBRegressor(n_estimators=1000, seed=0)\n",
    "  model.fit(X_t1, Y_train)\n",
    "  Y_pred = model.predict(X_t2)\n",
    "  print str(pt)+\"%\", mean_absolute_error(np.expm1(Y_test), np.expm1(Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we generate prediction result for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125546, 2)\n",
      "   id        loss\n",
      "0   4  148.563919\n",
      "1   6  148.563919\n",
      "2   9  328.922943\n",
      "3  12  307.540894\n",
      "4  15  133.955414\n"
     ]
    }
   ],
   "source": [
    "selector = SelectPercentile(f_regression, percentile= 75)\n",
    "selector.fit(X, Y)\n",
    "X_t1 = selector.transform(X)\n",
    "X_t2 = selector.transform(test.values)\n",
    "\n",
    "n_estimators = 10\n",
    "model = XGBRegressor(n_estimators=n_estimators,seed=0)\n",
    "model.fit(X_t1, Y)\n",
    "\n",
    "y_test = model.predict(X_t2)\n",
    "y_test = np.expm1(y_test)\n",
    "\n",
    "df = pd.concat([id_test, pd.Series(y_test)], axis = 1)\n",
    "df.columns = ['id', 'loss']\n",
    "\n",
    "print df.shape\n",
    "print df.head(5)\n",
    "\n",
    "df.to_csv('submission3.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
